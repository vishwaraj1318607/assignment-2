{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj5mlMzSDsFW",
        "outputId": "19c93198-eafe-4725-84d2-88a892dfdfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a,', 1), ('and,', 1), ('as', 1), ('be', 1), ('common', 1), ('contains', 1), ('input', 1), ('of,', 1), ('output.', 1), ('removed', 1), ('sample', 1), ('should', 1), ('some', 1), ('stopwords', 1), ('such', 1), ('text.', 1), ('the,', 1), ('these', 1), ('this', 1), ('to.', 1), ('words', 1)]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "# Mapper function\n",
        "def map(doc_id, text):\n",
        "    stopwords = set([\"the\", \"and\", \"of\", \"a\", \"to\", \"in\", \"is\", \"it\"])\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word.lower() not in stopwords:\n",
        "            yield (word.lower(), 1)\n",
        "\n",
        "# Reducer function\n",
        "def reduce(word, counts):\n",
        "    total_count = sum(counts)\n",
        "    yield (word, total_count)\n",
        "\n",
        "# Read the content of the input file\n",
        "def read_input_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    return content\n",
        "\n",
        "# Main function to run the MapReduce program\n",
        "def main(file_path):\n",
        "    input_text = read_input_file(file_path)\n",
        "    # Assuming each line in the input file is a separate document\n",
        "    documents = input_text.splitlines()\n",
        "\n",
        "    # Mapper phase\n",
        "    intermediate_output = []\n",
        "    for doc_id, text in enumerate(documents):\n",
        "        intermediate_output.extend(map(doc_id, text))\n",
        "\n",
        "    # Sort and group intermediate output by keys\n",
        "    intermediate_output.sort(key=lambda x: x[0])\n",
        "    grouped_output = {}\n",
        "    for key, group in itertools.groupby(intermediate_output, key=lambda x: x[0]):\n",
        "        grouped_output[key] = [count for _, count in group]\n",
        "\n",
        "    # Reducer phase\n",
        "    final_output = []\n",
        "    for word, counts in grouped_output.items():\n",
        "        final_output.extend(reduce(word, counts))\n",
        "\n",
        "    return final_output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = r\"/content/sample_data/1.txt\"\n",
        "    result = main(input_file_path)\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhHfLlKVFGWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TfBpIYGFIfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}