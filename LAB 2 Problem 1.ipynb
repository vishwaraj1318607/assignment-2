{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-w_vaWsAVGK",
        "outputId": "4b753d6b-25da-4e3c-f2d4-bfc2ed5770e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"lorem\" 1\n",
            "\"ipsum\" 1\n",
            "\"dolor\" 1\n",
            "\"sit\" 2\n",
            "\"amet\" 2\n",
            "\"consectetur\" 1\n",
            "\"adipiscing\" 1\n",
            "\"elit\" 3\n",
            "\"donec\" 1\n",
            "\"condimentum\" 1\n",
            "\"vel\" 1\n",
            "\"mauris\" 1\n",
            "\"varius\" 2\n",
            "\"id\" 1\n",
            "\"laoreet\" 1\n",
            "\"tortor\" 1\n",
            "\"placerat\" 1\n",
            "\"nulla\" 1\n",
            "\"scelerisque\" 1\n",
            "\"felis\" 1\n",
            "\"ac\" 1\n",
            "\"risus\" 1\n",
            "\"luctus\" 1\n",
            "\"mattis\" 1\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "def mapper(file_path):\n",
        "    # Read the input file and tokenize the words\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            words = re.findall(r'\\w+', line.lower())\n",
        "            for word in words:\n",
        "                yield (word, 1)\n",
        "\n",
        "def reducer(word, counts):\n",
        "    # Sum the counts of each word\n",
        "    return word, sum(counts)\n",
        "\n",
        "def map_reduce(input_file):\n",
        "    # Step 1: Map phase\n",
        "    mapped_data = defaultdict(list)\n",
        "    for word, count in mapper(input_file):\n",
        "        mapped_data[word].append(count)\n",
        "\n",
        "    # Step 2: Shuffle and Sort (not explicitly coded, as it's handled automatically in the background by MapReduce frameworks)\n",
        "\n",
        "    # Step 3: Reduce phase\n",
        "    for word, counts in mapped_data.items():\n",
        "        yield reducer(word, counts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/sample_data/input.txt\"\n",
        "    unique_word_counts = list(map_reduce(input_file))\n",
        "\n",
        "    # Print the output in the desired format\n",
        "    for word, count in unique_word_counts:\n",
        "        print(f'\"{word}\" {count}')\n"
      ]
    }
  ]
}